{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a68b0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajoute la racine du projet (celle qui contient 'src') au sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbc0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.image_utils import encode_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0efefc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing.mistral_ocr_llm import image_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1379f15f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OCRPageObject' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m image_path = \u001b[33m\"\u001b[39m\u001b[33m/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/tmp/paper/detection_20251015-171658-182_q0.jpg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m base64_image = encode_image(image_path)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mimage_transcription\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase64_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/p1-dty-rte/detection-notes/src/processing/mistral_ocr_llm.py:94\u001b[39m, in \u001b[36mimage_transcription\u001b[39m\u001b[34m(base64_image)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimage_transcription\u001b[39m(base64_image: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# 1. OCR brut\u001b[39;00m\n\u001b[32m     86\u001b[39m     response = client.ocr.process(\n\u001b[32m     87\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mmistral-ocr-latest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m         document={\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m         include_image_base64=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     93\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     ocr_text = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m.strip()\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== OCR brut ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, ocr_text)\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# 2. Pré-process\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/p1-dty-rte/detection-notes/.venv/lib/python3.13/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'OCRPageObject' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "image_path = \"/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/tmp/paper/detection_20251015-171658-182_q0.jpg\"\n",
    "base64_image = encode_image(image_path)\n",
    "image_transcription(base64_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997c6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing.add_data2db import add_data2db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e800291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV\n",
      "- Ancienne ligne 3. MNV à réaliser SUAV\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV'}, {'type': 'delete', 'old_line': 3, 'old_content': 'MNV à réaliser SUAV'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 4)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV\n",
      "- Ancienne ligne 3. MNV à réaliser SUAV\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV'}, {'type': 'delete', 'old_line': 3, 'old_content': 'MNV à réaliser SUAV'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV\n",
      "- Ancienne ligne 3. MNV à réaliser SUAV\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV'}, {'type': 'delete', 'old_line': 3, 'old_content': 'MNV à réaliser SUAV'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 4)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV\n",
      "- Ancienne ligne 3. MNV à réaliser SUAV\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV'}, {'type': 'delete', 'old_line': 3, 'old_content': 'MNV à réaliser SUAV'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data2db(\"/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/data/images_scenarios/IMG_3416.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c14e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 5)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 5)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data2db(\"/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/data/images_scenarios/IMG_3417.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0daf7214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "+ Ligne 3. MNV à réaliser SUAV\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV'}, {'type': 'insert', 'line': 3, 'content': 'MNV à réaliser SUAV'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 6)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "+ Ligne 3. MNV à réaliser SUAV\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV'}, {'type': 'insert', 'line': 3, 'content': 'MNV à réaliser SUAV'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "+ Ligne 3. MNV à réaliser SUAV\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV'}, {'type': 'insert', 'line': 3, 'content': 'MNV à réaliser SUAV'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 6)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "+ Ligne 3. MNV à réaliser SUAV\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV'}, {'type': 'insert', 'line': 3, 'content': 'MNV à réaliser SUAV'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data2db(\"/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/data/images_scenarios/IMG_3418.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63db2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "def _normalize_for_similarity(s: str) -> str:\n",
    "    # Minimise l'effet de variations typographiques mineures\n",
    "    s = s.strip().lower()\n",
    "\n",
    "    # Normalise les tirets et espaces autour des signes - : ; , .\n",
    "    s = re.sub(r\"\\s*[-–—]\\s*\", \"-\", s)       # \"Caen - Cherbourg\" -> \"caen-cherbourg\"\n",
    "    s = re.sub(r\"\\s*:\\s*\", \":\", s)\n",
    "    s = re.sub(r\"\\s*;\\s*\", \";\", s)\n",
    "    s = re.sub(r\"\\s*,\\s*\", \",\", s)\n",
    "    s = re.sub(r\"\\s*\\.\\s*\", \".\", s)\n",
    "\n",
    "    # Écrase espaces multiples\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _similarity(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, _normalize_for_similarity(a), _normalize_for_similarity(b)).ratio()\n",
    "\n",
    "def _align_block(old_block: List[str], new_block: List[str]) -> Tuple[List[Tuple[int,int,float]], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Aligne un bloc old_block (indexés par i) et new_block (indexés par j) par similarité.\n",
    "    Retourne:\n",
    "      - matches: liste de (i, j, sim) appariés (greedy, sim décroissante)\n",
    "      - old_unmatched: indices i non appariés (à supprimer)\n",
    "      - new_unmatched: indices j non appariés (à insérer)\n",
    "    \"\"\"\n",
    "    # calcule toutes les paires avec leur similarité\n",
    "    pairs = []\n",
    "    for i, a in enumerate(old_block):\n",
    "        for j, b in enumerate(new_block):\n",
    "            sim = _similarity(a, b)\n",
    "            pairs.append((sim, i, j))\n",
    "    # tri décroissant par similarité\n",
    "    pairs.sort(reverse=True, key=lambda t: t[0])\n",
    "\n",
    "    matched_old = set()\n",
    "    matched_new = set()\n",
    "    matches = []\n",
    "\n",
    "    for sim, i, j in pairs:\n",
    "        if i in matched_old or j in matched_new:\n",
    "            continue\n",
    "        # On accepte un match même si sim est faible ; on décidera ensuite s’il faut le logguer\n",
    "        matched_old.add(i)\n",
    "        matched_new.add(j)\n",
    "        matches.append((i, j, sim))\n",
    "\n",
    "    old_unmatched = [i for i in range(len(old_block)) if i not in matched_old]\n",
    "    new_unmatched = [j for j in range(len(new_block)) if j not in matched_new]\n",
    "    # tri par indices croissants pour stabilité\n",
    "    matches.sort(key=lambda t: (t[1], t[0]), reverse=False)  # principalement par j (ordre des nouvelles lignes)\n",
    "    return matches, old_unmatched, new_unmatched\n",
    "\n",
    "def compute_diff(old_text: str,\n",
    "                 new_text: str,\n",
    "                 minor_change_threshold: float = 0.90) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Renvoie (human_str, diff_json)\n",
    "    - human_str : lignes ajoutées / modifiées / supprimées (monospace) avec n° de ligne\n",
    "    - diff_json : liste d'opérations {type, line, content, ...}\n",
    "      type ∈ {\"insert\",\"replace\",\"delete\"}\n",
    "      line = n° de ligne dans le NOUVEAU texte (1-based) pour insert/replace,\n",
    "             n° de ligne dans l’ANCIEN pour delete (clé 'old_line').\n",
    "    Règles :\n",
    "      - insert : toujours listé\n",
    "      - replace : listé seulement si différence significative (similarité < minor_change_threshold)\n",
    "      - delete : toujours listé\n",
    "    \"\"\"\n",
    "    old_lines = old_text.splitlines()\n",
    "    new_lines = new_text.splitlines()\n",
    "\n",
    "    sm = SequenceMatcher(None, old_lines, new_lines, autojunk=False)\n",
    "\n",
    "    human_rows: List[str] = []\n",
    "    diff_json: List[Dict] = []\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"equal\":\n",
    "            continue\n",
    "\n",
    "        old_block = old_lines[i1:i2]\n",
    "        new_block = new_lines[j1:j2]\n",
    "\n",
    "        if tag in (\"replace\", \"insert\", \"delete\"):\n",
    "            # Aligne intelligemment, même si tailles identiques\n",
    "            matches, old_unmatched, new_unmatched = _align_block(old_block, new_block)\n",
    "\n",
    "            # 1) REPLACE (pour les paires appariées)\n",
    "            for i_rel, j_rel, sim in matches:\n",
    "                old_content = old_block[i_rel]\n",
    "                new_content = new_block[j_rel]\n",
    "                new_abs_line = j1 + j_rel + 1      # 1-based dans le nouveau texte\n",
    "                old_abs_line = i1 + i_rel + 1      # 1-based dans l’ancien texte\n",
    "\n",
    "                # Ne journalise pas si c'est un changement mineur\n",
    "                if _normalize_for_similarity(old_content) == _normalize_for_similarity(new_content):\n",
    "                    # équivalent “tolérant” → rien\n",
    "                    continue\n",
    "\n",
    "                if sim < minor_change_threshold:\n",
    "                    human_rows.append(f\"~ Ligne {new_abs_line}. {new_content}\")\n",
    "                    diff_json.append({\n",
    "                        \"type\": \"replace\",\n",
    "                        \"line\": new_abs_line,\n",
    "                        \"old_line\": old_abs_line,\n",
    "                        \"old_content\": old_content,\n",
    "                        \"content\": new_content,\n",
    "                        \"similarity\": float(sim)\n",
    "                    })\n",
    "                # sinon (sim >= seuil) → on considère que c'est mineur → pas de log\n",
    "\n",
    "            # 2) INSERT (nouvelles lignes non appariées)\n",
    "            for j_rel in new_unmatched:\n",
    "                new_content = new_block[j_rel]\n",
    "                if not new_content.strip():\n",
    "                    continue\n",
    "                new_abs_line = j1 + j_rel + 1\n",
    "                human_rows.append(f\"+ Ligne {new_abs_line}. {new_content}\")\n",
    "                diff_json.append({\n",
    "                    \"type\": \"insert\",\n",
    "                    \"line\": new_abs_line,\n",
    "                    \"content\": new_content\n",
    "                })\n",
    "\n",
    "            # 3) DELETE (anciennes lignes non appariées)\n",
    "            for i_rel in old_unmatched:\n",
    "                old_content = old_block[i_rel]\n",
    "                if not old_content.strip():\n",
    "                    continue\n",
    "                old_abs_line = i1 + i_rel + 1\n",
    "                human_rows.append(f\"- Ancienne ligne {old_abs_line}. {old_content}\")\n",
    "                diff_json.append({\n",
    "                    \"type\": \"delete\",\n",
    "                    \"old_line\": old_abs_line,\n",
    "                    \"old_content\": old_content\n",
    "                })\n",
    "\n",
    "    human_str = \"\\n\".join(human_rows)\n",
    "    return human_str, diff_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c1c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "# ----------------------------- #\n",
    "#  Utils similarité / matching  #\n",
    "# ----------------------------- #\n",
    "\n",
    "def _normalize_for_similarity(s: str) -> str:\n",
    "    # Minimise l'effet de variations typographiques mineures\n",
    "    s = s.strip().lower()\n",
    "\n",
    "    # Normalise les tirets et espaces autour des signes - : ; , .\n",
    "    s = re.sub(r\"\\s*[-–—]\\s*\", \"-\", s)       # \"Caen - Cherbourg\" -> \"caen-cherbourg\"\n",
    "    s = re.sub(r\"\\s*:\\s*\", \":\", s)\n",
    "    s = re.sub(r\"\\s*;\\s*\", \";\", s)\n",
    "    s = re.sub(r\"\\s*,\\s*\", \",\", s)\n",
    "    s = re.sub(r\"\\s*\\.\\s*\", \".\", s)\n",
    "\n",
    "    # Écrase espaces multiples\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _similarity(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, _normalize_for_similarity(a), _normalize_for_similarity(b)).ratio()\n",
    "\n",
    "def _similarity_concat(a: str, b1: str, b2: str) -> float:\n",
    "    return _similarity(a, f\"{b1} {b2}\".strip())\n",
    "\n",
    "def _align_block(old_block: List[str], new_block: List[str]) -> Tuple[List[Tuple[int,int,float]], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Aligne un bloc old_block (indexés par i) et new_block (indexés par j) par similarité.\n",
    "    Retourne:\n",
    "      - matches: liste de (i, j, sim) appariés (greedy, sim décroissante)\n",
    "      - old_unmatched: indices i non appariés (à supprimer)\n",
    "      - new_unmatched: indices j non appariés (à insérer)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i, a in enumerate(old_block):\n",
    "        for j, b in enumerate(new_block):\n",
    "            sim = _similarity(a, b)\n",
    "            pairs.append((sim, i, j))\n",
    "    pairs.sort(reverse=True, key=lambda t: t[0])\n",
    "\n",
    "    matched_old = set()\n",
    "    matched_new = set()\n",
    "    matches = []\n",
    "    for sim, i, j in pairs:\n",
    "        if i in matched_old or j in matched_new:\n",
    "            continue\n",
    "        matched_old.add(i)\n",
    "        matched_new.add(j)\n",
    "        matches.append((i, j, sim))\n",
    "\n",
    "    old_unmatched = [i for i in range(len(old_block)) if i not in matched_old]\n",
    "    new_unmatched = [j for j in range(len(new_block)) if j not in matched_new]\n",
    "\n",
    "    # tri par ordre d'apparition côté NEW pour stabilité\n",
    "    matches.sort(key=lambda t: (t[1], t[0]))\n",
    "    return matches, old_unmatched, new_unmatched\n",
    "\n",
    "def _try_split_merge_matches(old_block: List[str], new_block: List[str], split_thresh: float = 0.85):\n",
    "    \"\"\"\n",
    "    Détecte localement des splits (1 old -> 2 new) et merges (2 old -> 1 new).\n",
    "    Retourne:\n",
    "      - matches: liste de tuples ((i_start,i_end), (j_start,j_end), kind) avec kind in {\"1to2\",\"2to1\"}\n",
    "      - used_old: set d'indices old consommés\n",
    "      - used_new: set d'indices new consommés\n",
    "    \"\"\"\n",
    "    n_old, n_new = len(old_block), len(new_block)\n",
    "    used_old, used_new = set(), set()\n",
    "    matches = []\n",
    "\n",
    "    # --- 1 -> 2 (split)\n",
    "    for i in range(n_old):\n",
    "        if i in used_old:\n",
    "            continue\n",
    "        best = None\n",
    "        for j in range(n_new - 1):\n",
    "            if j in used_new or (j + 1) in used_new:\n",
    "                continue\n",
    "            sim = _similarity_concat(old_block[i], new_block[j], new_block[j + 1])\n",
    "            if best is None or sim > best[0]:\n",
    "                best = (sim, i, j)\n",
    "        if best and best[0] >= split_thresh:\n",
    "            sim, i0, j0 = best\n",
    "            matches.append(((i0, i0), (j0, j0 + 1), \"1to2\"))\n",
    "            used_old.add(i0)\n",
    "            used_new.update({j0, j0 + 1})\n",
    "\n",
    "    # --- 2 -> 1 (merge)\n",
    "    for j in range(n_new):\n",
    "        if j in used_new:\n",
    "            continue\n",
    "        best = None\n",
    "        for i in range(n_old - 1):\n",
    "            if i in used_old or (i + 1) in used_old:\n",
    "                continue\n",
    "            sim = _similarity_concat(new_block[j], old_block[i], old_block[i + 1])\n",
    "            if best is None or sim > best[0]:\n",
    "                best = (sim, i, j)\n",
    "        if best and best[0] >= split_thresh:\n",
    "            sim, i0, j0 = best\n",
    "            matches.append(((i0, i0 + 1), (j0, j0), \"2to1\"))\n",
    "            used_old.update({i0, i0 + 1})\n",
    "            used_new.add(j0)\n",
    "\n",
    "    matches.sort(key=lambda m: m[1][0])  # ordre d’apparition côté NEW\n",
    "    return matches, used_old, used_new\n",
    "\n",
    "# ----------------------------- #\n",
    "#            DIFF               #\n",
    "# ----------------------------- #\n",
    "\n",
    "def compute_diff(old_text: str,\n",
    "                 new_text: str,\n",
    "                 minor_change_threshold: float = 0.90,\n",
    "                 split_merge_threshold: float = 0.85) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Renvoie (human_str, diff_json)\n",
    "    - human_str : lignes ajoutées / modifiées / supprimées (monospace) avec n° de ligne\n",
    "    - diff_json : liste d'opérations {type, line, content, ...}\n",
    "      type ∈ {\"insert\",\"replace\",\"delete\"}\n",
    "      line = n° de ligne dans le NOUVEAU texte (1-based) pour insert/replace,\n",
    "             n° de ligne dans l’ANCIEN pour delete (clé 'old_line').\n",
    "    Règles :\n",
    "      - insert : toujours listé\n",
    "      - replace : listé seulement si différence significative (similarité < minor_change_threshold)\n",
    "      - delete : toujours listé\n",
    "      - split/merge : tentatives avant alignement 1↔1 pour éviter des inserts artificiels\n",
    "    \"\"\"\n",
    "    old_lines = old_text.splitlines()\n",
    "    new_lines = new_text.splitlines()\n",
    "\n",
    "    sm = SequenceMatcher(None, old_lines, new_lines, autojunk=False)\n",
    "\n",
    "    human_rows: List[str] = []\n",
    "    diff_json: List[Dict] = []\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"equal\":\n",
    "            continue\n",
    "\n",
    "        old_block = old_lines[i1:i2]\n",
    "        new_block = new_lines[j1:j2]\n",
    "\n",
    "        if tag in (\"replace\", \"insert\", \"delete\"):\n",
    "            # ---- 0) Détection split/merge en amont\n",
    "            split_matches, used_old, used_new = _try_split_merge_matches(\n",
    "                old_block, new_block, split_thresh=split_merge_threshold\n",
    "            )\n",
    "\n",
    "            # Émettre les remplacements pour les matches 1→2 et 2→1 (sans inserts/deletes)\n",
    "            for (i_start, i_end), (j_start, j_end), kind in split_matches:\n",
    "                if kind == \"1to2\":\n",
    "                    a = old_block[i_start]\n",
    "                    b1, b2 = new_block[j_start], new_block[j_start + 1]\n",
    "\n",
    "                    sim1 = _similarity(a, b1)\n",
    "                    if _normalize_for_similarity(a) != _normalize_for_similarity(b1) and sim1 < minor_change_threshold and b1.strip():\n",
    "                        line_new = j1 + j_start + 1\n",
    "                        human_rows.append(f\"~ Ligne {line_new}. {b1}\")\n",
    "                        diff_json.append({\n",
    "                            \"type\": \"replace\",\n",
    "                            \"line\": line_new,\n",
    "                            \"old_line\": i1 + i_start + 1,\n",
    "                            \"old_content\": a,\n",
    "                            \"content\": b1,\n",
    "                            \"similarity\": float(sim1),\n",
    "                            \"note\": \"split(1→2)-part1\"\n",
    "                        })\n",
    "\n",
    "                    sim2 = _similarity(a, b2)\n",
    "                    if _normalize_for_similarity(a) != _normalize_for_similarity(b2) and sim2 < minor_change_threshold and b2.strip():\n",
    "                        line_new = j1 + j_start + 2\n",
    "                        human_rows.append(f\"~ Ligne {line_new}. {b2}\")\n",
    "                        diff_json.append({\n",
    "                            \"type\": \"replace\",\n",
    "                            \"line\": line_new,\n",
    "                            \"old_line\": i1 + i_start + 1,\n",
    "                            \"old_content\": a,\n",
    "                            \"content\": b2,\n",
    "                            \"similarity\": float(sim2),\n",
    "                            \"note\": \"split(1→2)-part2\"\n",
    "                        })\n",
    "\n",
    "                elif kind == \"2to1\":\n",
    "                    a1, a2 = old_block[i_start], old_block[i_end]\n",
    "                    b = new_block[j_start]\n",
    "                    sim = _similarity(f\"{a1} {a2}\".strip(), b)\n",
    "                    if _normalize_for_similarity(f\"{a1} {a2}\") != _normalize_for_similarity(b) and sim < minor_change_threshold and b.strip():\n",
    "                        line_new = j1 + j_start + 1\n",
    "                        human_rows.append(f\"~ Ligne {line_new}. {b}\")\n",
    "                        diff_json.append({\n",
    "                            \"type\": \"replace\",\n",
    "                            \"line\": line_new,\n",
    "                            \"old_line\": [i1 + i_start + 1, i1 + i_end + 1],\n",
    "                            \"old_content\": f\"{a1} {a2}\",\n",
    "                            \"content\": b,\n",
    "                            \"similarity\": float(sim),\n",
    "                            \"note\": \"merge(2→1)\"\n",
    "                        })\n",
    "\n",
    "            # ---- 1) Enlève ce qui a été “consommé” par split/merge\n",
    "            old_rest_map = [k for k in range(len(old_block)) if k not in used_old]\n",
    "            new_rest_map = [k for k in range(len(new_block)) if k not in used_new]\n",
    "            old_rest = [old_block[k] for k in old_rest_map]\n",
    "            new_rest = [new_block[k] for k in new_rest_map]\n",
    "\n",
    "            # ---- 2) Aligne le reste 1↔1\n",
    "            matches, old_unmatched_rel, new_unmatched_rel = _align_block(old_rest, new_rest)\n",
    "\n",
    "            # Remappe indices relatifs vers indices initiaux de block\n",
    "            remapped_matches = []\n",
    "            for i_rel2, j_rel2, sim in matches:\n",
    "                i_rel_orig = old_rest_map[i_rel2] if i_rel2 < len(old_rest_map) else None\n",
    "                j_rel_orig = new_rest_map[j_rel2] if j_rel2 < len(new_rest_map) else None\n",
    "                remapped_matches.append((i_rel_orig, j_rel_orig, sim))\n",
    "\n",
    "            old_unmatched = [old_rest_map[i] for i in old_unmatched_rel]\n",
    "            new_unmatched = [new_rest_map[j] for j in new_unmatched_rel]\n",
    "\n",
    "            # ---- 3) REPLACE pour les paires appariées restantes\n",
    "            for i_rel, j_rel, sim in remapped_matches:\n",
    "                old_content = old_block[i_rel]\n",
    "                new_content = new_block[j_rel]\n",
    "                new_abs_line = j1 + j_rel + 1\n",
    "                old_abs_line = i1 + i_rel + 1\n",
    "\n",
    "                if _normalize_for_similarity(old_content) == _normalize_for_similarity(new_content):\n",
    "                    continue  # changement mineur “tolérant”\n",
    "\n",
    "                if sim < minor_change_threshold:\n",
    "                    human_rows.append(f\"~ Ligne {new_abs_line}. {new_content}\")\n",
    "                    diff_json.append({\n",
    "                        \"type\": \"replace\",\n",
    "                        \"line\": new_abs_line,\n",
    "                        \"old_line\": old_abs_line,\n",
    "                        \"old_content\": old_content,\n",
    "                        \"content\": new_content,\n",
    "                        \"similarity\": float(sim)\n",
    "                    })\n",
    "                # sinon (sim >= seuil) => mineur => pas de log\n",
    "\n",
    "            # ---- 4) INSERT pour les nouvelles lignes non appariées\n",
    "            for j_rel in new_unmatched:\n",
    "                new_content = new_block[j_rel]\n",
    "                if not new_content.strip():\n",
    "                    continue\n",
    "                new_abs_line = j1 + j_rel + 1\n",
    "                human_rows.append(f\"+ Ligne {new_abs_line}. {new_content}\")\n",
    "                diff_json.append({\n",
    "                    \"type\": \"insert\",\n",
    "                    \"line\": new_abs_line,\n",
    "                    \"content\": new_content\n",
    "                })\n",
    "\n",
    "            # ---- 5) DELETE pour les anciennes lignes non appariées\n",
    "            for i_rel in old_unmatched:\n",
    "                old_content = old_block[i_rel]\n",
    "                if not old_content.strip():\n",
    "                    continue\n",
    "                old_abs_line = i1 + i_rel + 1\n",
    "                human_rows.append(f\"- Ancienne ligne {old_abs_line}. {old_content}\")\n",
    "                diff_json.append({\n",
    "                    \"type\": \"delete\",\n",
    "                    \"old_line\": old_abs_line,\n",
    "                    \"old_content\": old_content\n",
    "                })\n",
    "\n",
    "    human_str = \"\\n\".join(human_rows)\n",
    "    return human_str, diff_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9212b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV \n",
    "Analyse des audits Cherbourg\"\"\"\n",
    "\n",
    "text2 = \"\"\"Confirmé 1 MNV SNCF Nantes-Vannes \n",
    "Étude RDCR Gouin Cherbourg 225kV \n",
    "Analyse des audios Cherbourg\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f936e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Résumé humain du diff :\n",
      "\n",
      "~ Ligne 1. Confirmé 1 MNV SNCF Nantes-Vannes \n",
      "~ Ligne 2. Étude RDCR Gouin Cherbourg 225kV \n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test du diff avec seuil d'ignorance des changements mineurs\n",
    "human, diff = compute_diff(text1, text2, minor_change_threshold=0.9)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Résumé humain du diff :\\n\")\n",
    "if human.strip():\n",
    "    print(human)\n",
    "else:\n",
    "    print(\"Aucune différence significative détectée.\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96bd3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap, re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a738283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_in_line(text: str, line_index: int) -> int:\n",
    "    \"\"\"\n",
    "    Retourne le nombre de caractères dans la ligne `line_index` du texte.\n",
    "    - `text` : chaîne potentiellement multi-lignes\n",
    "    - `line_index` : index 1-based (1 = première ligne)\n",
    "    Lève IndexError si line_index hors limites.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    if line_index < 1 or line_index > len(lines):\n",
    "        raise IndexError(f\"line_index {line_index} hors limites (1..{len(lines)})\")\n",
    "    return len(lines[line_index - 1])\n",
    "\n",
    "def chars_per_line(text: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Retourne une liste contenant le nombre de caractères pour chaque ligne du texte.\n",
    "    \"\"\"\n",
    "    return [len(ln) for ln in text.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 =\"\"\"Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV\n",
    "Analyse des audios Cherbourg\n",
    "Changement de numéro +33663462732\n",
    "Penser à basculer sur le N-1 de Strasbourg-Colmar à 14h31\"\"\"\n",
    "text2 =\"\"\"Cordilomon la MNV SNCF Nantes-Vannes\n",
    "Étude RDCR Caen Cherbourg 225kV\n",
    "Analyse des audios Cherbourg\n",
    "Changement de numéro +33663962732\n",
    "Penser à basculer sur le N-1 de Strasbourg-Colmar à 14h31\"\"\"\n",
    "text3 =\"\"\"Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV\n",
    "Analyse des audios Cherbourg\n",
    "Changement de numéro +33663462732\n",
    "Penser à basculer sur le N-1 de Strasbourg-Colmar à 14h31\"\"\"\n",
    "text4 =\"\"\"Conformes la MNV SNCF Nantes-Vannes\n",
    "Étude RDCR Caen Cherbourg 225kV\n",
    "Analyse des audios Cherbourg\n",
    "Changement de numéro +33663962732\n",
    "Penser à basculer sur le N-1 de Strasbourg-Colmar à 14h31\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbf2afeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 28, 33, 57]\n",
      "[77, 78, 35]\n",
      "[36, 31, 28, 33, 57]\n",
      "[79, 80, 32]\n",
      "[67, 28, 33, 57]\n",
      "[77, 79, 35]\n",
      "[35, 31, 28, 33, 57]\n",
      "[78, 79, 35]\n"
     ]
    }
   ],
   "source": [
    "text1_corr =\"\"\"Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV. Analyse \n",
    "des audios Cherbourg. Changement de numéro +33663462732. Penser à basculer sur\n",
    "le N-1 de Strasbourg-Colmar à 14h31\"\"\"\n",
    "text2_corr =\"\"\"Cordilomon la MNV SNCF Nantes-Vannes. Étude RDCR Caen Cherbourg 225kV. Analyse \n",
    "des audios CherbourgChangement de numéro +33663962732. Penser à basculer sur le \n",
    "N-1 de Strasbourg-Colmar à 14h31\"\"\"\n",
    "text3_corr =\"\"\"Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV. Analyse \n",
    "des audios Cherbourg. Changement de numéro +33663462732. Penser à basculer sur \n",
    "le N-1 de Strasbourg-Colmar à 14h31\"\"\"\n",
    "text4_corr =\"\"\"Conformes la MNV SNCF Nantes-Vannes. Étude RDCR Caen Cherbourg 225kV. Analyse \n",
    "des audios Cherbourg. Changement de numéro +33663962732. Penser à basculer sur \n",
    "le N-1 de Strasbourg-Colmar à 14h31\"\"\"\n",
    "\n",
    "print(chars_per_line(text1))\n",
    "print(chars_per_line(text1_corr))\n",
    "print(chars_per_line(text2))\n",
    "print(chars_per_line(text2_corr))\n",
    "print(chars_per_line(text3))\n",
    "print(chars_per_line(text3_corr))\n",
    "print(chars_per_line(text4))\n",
    "print(chars_per_line(text4_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "177ed7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- text1 ---\n",
      "Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV. Analyse des\n",
      "audios Cherbourg. Changement de numéro +33663462732. Penser à basculer sur le\n",
      "N-1 de Strasbourg-Colmar à 14h31.\n",
      "chars_per_line: [80, 77, 33]\n",
      "\n",
      "--- text2 ---\n",
      "Cordilomon la MNV SNCF Nantes-Vannes. Étude RDCR Caen Cherbourg 225kV. Analyse\n",
      "des audios Cherbourg. Changement de numéro +33663962732. Penser à basculer sur\n",
      "le N-1 de Strasbourg-Colmar à 14h31.\n",
      "chars_per_line: [78, 78, 36]\n",
      "\n",
      "--- text3 ---\n",
      "Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV. Analyse des\n",
      "audios Cherbourg. Changement de numéro +33663462732. Penser à basculer sur le\n",
      "N-1 de Strasbourg-Colmar à 14h31.\n",
      "chars_per_line: [80, 77, 33]\n",
      "\n",
      "--- text4 ---\n",
      "Conformes la MNV SNCF Nantes-Vannes. Étude RDCR Caen Cherbourg 225kV. Analyse\n",
      "des audios Cherbourg. Changement de numéro +33663962732. Penser à basculer sur\n",
      "le N-1 de Strasbourg-Colmar à 14h31.\n",
      "chars_per_line: [77, 78, 36]\n",
      "\n",
      "--- text1_corr ---\n",
      "Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV. Analyse des\n",
      "audios Cherbourg. Changement de numéro +33663462732. Penser à basculer sur Le\n",
      "N-1 de Strasbourg-Colmar à 14h31.\n",
      "chars_per_line: [80, 77, 33]\n",
      "\n",
      "--- text2_corr ---\n",
      "Cordilomon la MNV SNCF Nantes-Vannes. Étude RDCR Caen Cherbourg 225kV. Analyse\n",
      "des audios CherbourgChangement de numéro +33663962732. Penser à basculer sur le\n",
      "N-1 de Strasbourg-Colmar à 14h31.\n",
      "chars_per_line: [78, 79, 33]\n",
      "\n",
      "--- text3_corr ---\n",
      "Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV. Analyse des\n",
      "audios Cherbourg. Changement de numéro +33663462732. Penser à basculer sur Le\n",
      "N-1 de Strasbourg-Colmar à 14h31.\n",
      "chars_per_line: [80, 77, 33]\n",
      "\n",
      "--- text4_corr ---\n",
      "Conformes la MNV SNCF Nantes-Vannes. Étude RDCR Caen Cherbourg 225kV. Analyse\n",
      "des audios Cherbourg. Changement de numéro +33663962732. Penser à basculer sur\n",
      "Le N-1 de Strasbourg-Colmar à 14h31.\n",
      "chars_per_line: [77, 78, 36]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def reflow_sentences(text: str, width: int = 80) -> str:\n",
    "    \"\"\"\n",
    "    Réarrange un texte composé de segments :\n",
    "    - n'ajoute un point qu'à la fin probable d'une phrase (ou si ponctuation existante),\n",
    "    - évite d'ajouter un point si le segment se termine par une préposition/mot court (ex: 'sur', 'à', 'le'),\n",
    "    - joint les segments non terminés au segment suivant si la ligne suivante commence par une minuscule,\n",
    "    - met une majuscule uniquement en début de phrase,\n",
    "    - wrappe à <= width caractères sans couper les mots.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import textwrap\n",
    "\n",
    "    if not text or not text.strip():\n",
    "        return text or \"\"\n",
    "\n",
    "    non_terminal_words = {\"sur\", \"à\", \"le\", \"la\", \"les\", \"des\", \"de\", \"du\", \"en\", \"et\", \"ou\", \"par\", \"pour\", \"avec\", \"au\", \"aux\", \"chez\", \"dans\", \"vers\"}\n",
    "\n",
    "    parts = [p.rstrip() for p in text.splitlines()]\n",
    "    # keep empty parts so we can reason about next-line existence\n",
    "\n",
    "    # normalize internal spaces\n",
    "    parts = [re.sub(r\"\\s+\", \" \", p).strip() for p in parts]\n",
    "\n",
    "    merged_parts = []\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        p = parts[i]\n",
    "        if not p:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # if already terminal, keep\n",
    "        if re.search(r\"[\\.\\?!]$\", p):\n",
    "            merged_parts.append(p)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # lookahead to next non-empty part\n",
    "        j = i + 1\n",
    "        next_part = None\n",
    "        while j < len(parts):\n",
    "            if parts[j]:\n",
    "                next_part = parts[j]\n",
    "                break\n",
    "            j += 1\n",
    "\n",
    "        last_word = p.split()[-1].lower() if p.split() else \"\"\n",
    "\n",
    "        # if next part exists and starts with a lowercase letter -> continuation\n",
    "        if next_part:\n",
    "            m = re.match(r\"\\s*([a-zà-ÿ])\", next_part, flags=re.IGNORECASE)\n",
    "            next_starts_lower = bool(m and m.group(1).islower())\n",
    "        else:\n",
    "            next_starts_lower = False\n",
    "\n",
    "        if last_word in non_terminal_words and next_part:\n",
    "            # join without adding a point\n",
    "            combined = p + \" \" + next_part\n",
    "            merged_parts.append(combined)\n",
    "            # skip the part we combined\n",
    "            i = j + 1\n",
    "            continue\n",
    "\n",
    "        if next_part and next_starts_lower:\n",
    "            # don't add a point, join to maintain lowercase start\n",
    "            combined = p + \" \" + next_part\n",
    "            merged_parts.append(combined)\n",
    "            i = j + 1\n",
    "            continue\n",
    "\n",
    "        # otherwise treat as terminal\n",
    "        merged_parts.append(p + \".\")\n",
    "        i += 1\n",
    "\n",
    "    # Capitalize only sentence starts; ensure single space after punctuation\n",
    "    def cap_sentence(s: str) -> str:\n",
    "        s = s.strip()\n",
    "        s = re.sub(r\"\\s*([\\.\\?!])\\s*\", lambda m: m.group(1) + \" \", s)\n",
    "        # Capitalize only the letter after sentence boundary\n",
    "        s = re.sub(r\"(^|[\\.\\?!]\\s+)([a-zà-ÿ])\", lambda m: m.group(1) + m.group(2).upper(), s, flags=re.IGNORECASE)\n",
    "        return s.strip()\n",
    "\n",
    "    sentences = [cap_sentence(s) for s in merged_parts if s.strip()]\n",
    "    # remove trailing spaces from each sentence before joining to avoid double spaces\n",
    "    paragraph = \" \".join(s.rstrip() for s in sentences)\n",
    "    wrapped = textwrap.fill(paragraph.strip(), width=width, break_long_words=False, break_on_hyphens=False)\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "# Ré-exécution des tests\n",
    "_examples = {\n",
    "    'text1': text1,\n",
    "    'text2': text2,\n",
    "    'text3': text3,\n",
    "    'text4': text4,\n",
    "    'text1_corr': text1_corr,\n",
    "    'text2_corr': text2_corr,\n",
    "    'text3_corr': text3_corr,\n",
    "    'text4_corr': text4_corr,\n",
    "}\n",
    "\n",
    "for name, val in _examples.items():\n",
    "    print('---', name, '---')\n",
    "    out = reflow_sentences(val, width=80)\n",
    "    print(out)\n",
    "    print('chars_per_line:', chars_per_line(out))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ed31c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Jaccard and F1:\n",
      "text1 : Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV. Analyse des\n",
      "audios Cherbourg. Changement de numéro +33663462732. Penser à basculer sur le\n",
      "N-1 de Strasbourg-Colmar à 14h31.\n",
      "text2 : Cordilomon la MNV SNCF Nantes-Vannes. Étude RDCR Caen Cherbourg 225kV. Analyse\n",
      "des audios Cherbourg. Changement de numéro +33663962732. Penser à basculer sur\n",
      "le N-1 de Strasbourg-Colmar à 14h31.\n",
      "text3 : Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV. Analyse des\n",
      "audios Cherbourg. Changement de numéro +33663462732. Penser à basculer sur le\n",
      "N-1 de Strasbourg-Colmar à 14h31.\n",
      "text4 : Conformes la MNV SNCF Nantes-Vannes. Étude RDCR Caen Cherbourg 225kV. Analyse\n",
      "des audios Cherbourg. Changement de numéro +33663962732. Penser à basculer sur\n",
      "le N-1 de Strasbourg-Colmar à 14h31.\n",
      "\n",
      "Matrix (jaccard, f1, equiv):\n",
      "text1 [(1.0, 1.0, True), (0.867, 0.935, True), (1.0, 1.0, True), (0.867, 0.935, True)]\n",
      "text2 [(0.867, 0.935, True), (1.0, 1.0, True), (0.867, 0.935, True), (0.931, 0.968, True)]\n",
      "text3 [(1.0, 1.0, True), (0.867, 0.935, True), (1.0, 1.0, True), (0.867, 0.935, True)]\n",
      "text4 [(0.867, 0.935, True), (0.931, 0.968, True), (0.867, 0.935, True), (1.0, 1.0, True)]\n",
      "\n",
      "All texts equivalent? -> True\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def canonicalize_for_compare(s: str) -> List[str]:\n",
    "    \"\"\"Return a list of normalized tokens for comparison.\n",
    "    - lowercased\n",
    "    - remove diacritics\n",
    "    - replace common punctuation by spaces\n",
    "    - keep numbers (phone numbers) as tokens\n",
    "    - split on whitespace\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    # normalize unicode\n",
    "    s = unicodedata.normalize('NFKD', s)\n",
    "    s = ''.join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = s.lower()\n",
    "    # replace punctuation (except + and digits) with spaces\n",
    "    s = re.sub(r\"[^0-9a-z+]+\", \" \", s)\n",
    "    tokens = [t for t in s.split() if t]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def token_jaccard(a: List[str], b: List[str]) -> float:\n",
    "    sa, sb = set(a), set(b)\n",
    "    if not sa and not sb:\n",
    "        return 1.0\n",
    "    inter = sa & sb\n",
    "    uni = sa | sb\n",
    "    return len(inter) / len(uni)\n",
    "\n",
    "\n",
    "def token_f1(a: List[str], b: List[str]) -> float:\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    ca, cb = Counter(a), Counter(b)\n",
    "    common = sum((ca & cb).values())\n",
    "    if common == 0:\n",
    "        return 0.0\n",
    "    prec = common / sum(ca.values())\n",
    "    rec = common / sum(cb.values())\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "\n",
    "def texts_equivalent(a: str, b: str, jaccard_thresh: float = 0.65, f1_thresh: float = 0.75) -> bool:\n",
    "    ta = canonicalize_for_compare(a)\n",
    "    tb = canonicalize_for_compare(b)\n",
    "    j = token_jaccard(ta, tb)\n",
    "    f = token_f1(ta, tb)\n",
    "    # special-case: if phone numbers differ only by 1-2 digits, still accept\n",
    "    # extract tokens that look like phone numbers\n",
    "    def phone_tokens(ts: List[str]):\n",
    "        return [t for t in ts if re.fullmatch(r\"\\+?\\d{6,}\", t)]\n",
    "    pa = phone_tokens(ta)\n",
    "    pb = phone_tokens(tb)\n",
    "    phone_ok = True\n",
    "    if pa and pb:\n",
    "        # compare normalized numbers ignoring separator differences\n",
    "        phone_ok = any(sum(c1!=c2 for c1,c2 in zip(x,y)) <= 2 for x in pa for y in pb)\n",
    "    return (j >= jaccard_thresh and f >= f1_thresh) or phone_ok\n",
    "\n",
    "\n",
    "# Run pairwise comparisons and print matrix\n",
    "texts = [reflow_sentences(text1, 80), reflow_sentences(text2, 80), reflow_sentences(text3, 80), reflow_sentences(text4, 80)]\n",
    "names = ['text1', 'text2', 'text3', 'text4']\n",
    "\n",
    "print('Pairwise Jaccard and F1:')\n",
    "mat = []\n",
    "for i,a in enumerate(texts):\n",
    "    row = []\n",
    "    for j,b in enumerate(texts):\n",
    "        ta = canonicalize_for_compare(a)\n",
    "        tb = canonicalize_for_compare(b)\n",
    "        ja = token_jaccard(ta,tb)\n",
    "        fa = token_f1(ta,tb)\n",
    "        eq = texts_equivalent(a,b)\n",
    "        row.append((round(ja,3), round(fa,3), eq))\n",
    "    mat.append(row)\n",
    "\n",
    "for i,name in enumerate(names):\n",
    "    print(name, ':', texts[i])\n",
    "print('\\nMatrix (jaccard, f1, equiv):')\n",
    "for i in range(len(names)):\n",
    "    print(names[i], mat[i])\n",
    "\n",
    "# Summary decision: are all texts pairwise equivalent?\n",
    "all_equiv = all(mat[i][j][2] for i in range(len(mat)) for j in range(len(mat)) if i!=j)\n",
    "print('\\nAll texts equivalent? ->', all_equiv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ee1d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score + category (pairwise):\n",
      "reflow_text1 vs reflow_text2 -> score=0.081 category=different (jaccard=0.061 f1=0.1 phone_ok=False)\n",
      "reflow_text1 vs reflow_text3 -> score=0.109 category=different (jaccard=0.083 f1=0.135 phone_ok=False)\n",
      "reflow_text1 vs reflow_text1bis -> score=0.95 category=identical (jaccard=0.93 f1=0.97 phone_ok=True)\n",
      "reflow_text1 vs reflow_text2bis -> score=0.054 category=different (jaccard=0.041 f1=0.068 phone_ok=False)\n",
      "reflow_text2 vs reflow_text3 -> score=0.042 category=different (jaccard=0.029 f1=0.056 phone_ok=False)\n",
      "reflow_text2 vs reflow_text1bis -> score=0.079 category=different (jaccard=0.06 f1=0.098 phone_ok=False)\n",
      "reflow_text2 vs reflow_text2bis -> score=0.583 category=related (jaccard=0.5 f1=0.667 phone_ok=False)\n",
      "reflow_text3 vs reflow_text1bis -> score=0.086 category=different (jaccard=0.065 f1=0.107 phone_ok=False)\n",
      "reflow_text3 vs reflow_text2bis -> score=0.0 category=different (jaccard=0.0 f1=0.0 phone_ok=False)\n",
      "reflow_text1bis vs reflow_text2bis -> score=0.053 category=different (jaccard=0.04 f1=0.067 phone_ok=False)\n"
     ]
    }
   ],
   "source": [
    "def score_and_categorize_texts(a: str, b: str, weights=(0.5, 0.5), thresholds=None) -> dict:\n",
    "    \"\"\"\n",
    "    Calcule un score continu [0,1] entre deux textes et retourne une catégorisation.\n",
    "\n",
    "    Retourne un dict: {\n",
    "      'score': float (0..1),\n",
    "      'category': str,  # one of 'identical','close','related','different'\n",
    "      'jaccard': float,\n",
    "      'f1': float,\n",
    "      'phone_ok': bool\n",
    "    }\n",
    "\n",
    "    Méthode:\n",
    "    - canonicalize tokens via `canonicalize_for_compare` (déjà dans le notebook),\n",
    "    - calcule Jaccard et token-F1,\n",
    "    - combine en score via `weights` (jaccard,f1),\n",
    "    - si les numéros de téléphone sont proches on booste le score,\n",
    "    - mappe le score sur des catégories par seuils.\n",
    "    \"\"\"\n",
    "    # defensive\n",
    "    thresholds = thresholds or {\"identical\": 0.90, \"close\": 0.75, \"related\": 0.50}\n",
    "\n",
    "    # normalize via existing helpers if available in the notebook namespace\n",
    "    try:\n",
    "        ta = canonicalize_for_compare(a or \"\")\n",
    "        tb = canonicalize_for_compare(b or \"\")\n",
    "    except NameError:\n",
    "        # fallback: simple tokenisation\n",
    "        import re\n",
    "        def _tok(s):\n",
    "            if not s:\n",
    "                return []\n",
    "            s = re.sub(r\"[^0-9a-z+]+\", \" \", s.lower())\n",
    "            return [t for t in s.split() if t]\n",
    "        ta = _tok(a)\n",
    "        tb = _tok(b)\n",
    "\n",
    "    # compute metrics\n",
    "    try:\n",
    "        ja = token_jaccard(ta, tb)\n",
    "    except NameError:\n",
    "        ja = 0.0\n",
    "    try:\n",
    "        fa = token_f1(ta, tb)\n",
    "    except NameError:\n",
    "        fa = 0.0\n",
    "\n",
    "    # phone tolerance (reuse logic from notebook if present)\n",
    "    def phone_tokens(ts):\n",
    "        import re\n",
    "        return [t for t in ts if re.fullmatch(r\"\\+?\\d{6,}\", t)]\n",
    "\n",
    "    pa = phone_tokens(ta)\n",
    "    pb = phone_tokens(tb)\n",
    "    phone_ok = False\n",
    "    if pa and pb:\n",
    "        # allow small digit differences\n",
    "        def close_nums(x, y):\n",
    "            # compare as strings, allow small mismatches up to 2 differing digits\n",
    "            z = zip(x, y)\n",
    "            diff = sum(1 for c1, c2 in z if c1 != c2)\n",
    "            diff += abs(len(x) - len(y))\n",
    "            return diff <= 2\n",
    "        phone_ok = any(close_nums(x, y) for x in pa for y in pb)\n",
    "\n",
    "    score = float(weights[0]) * ja + float(weights[1]) * fa\n",
    "    if phone_ok:\n",
    "        score = max(score, 0.70)\n",
    "\n",
    "    # clamp\n",
    "    score = max(0.0, min(1.0, score))\n",
    "\n",
    "    # categorization\n",
    "    if score >= thresholds[\"identical\"]:\n",
    "        cat = \"identical\"\n",
    "    elif score >= thresholds[\"close\"]:\n",
    "        cat = \"close\"\n",
    "    elif score >= thresholds[\"related\"]:\n",
    "        cat = \"related\"\n",
    "    else:\n",
    "        cat = \"different\"\n",
    "\n",
    "    return {\n",
    "        \"score\": round(score, 3),\n",
    "        \"category\": cat,\n",
    "        \"jaccard\": round(ja, 3),\n",
    "        \"f1\": round(fa, 3),\n",
    "        \"phone_ok\": phone_ok,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage (pairwise) using the existing `texts` list if present\n",
    "try:\n",
    "    _texts = texts\n",
    "    _names = names\n",
    "except NameError:\n",
    "    _texts = [reflow_sentences(text1, 80), reflow_sentences(text2, 80), reflow_sentences(text3, 80), reflow_sentences(text4, 80)]\n",
    "    _names = ['text1', 'text2', 'text3', 'text4']\n",
    "\n",
    "print('\\nScore + category (pairwise):')\n",
    "for i in range(len(_texts)):\n",
    "    for j in range(len(_texts)):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        res = score_and_categorize_texts(_texts[i], _texts[j])\n",
    "        print(f\"{_names[i]} vs {_names[j]} -> score={res['score']} category={res['category']} (jaccard={res['jaccard']} f1={res['f1']} phone_ok={res['phone_ok']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bacbc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"Incident: coupure focale 140kV (poste PN-2)\n",
    "MV à faire toutes les 15 min\n",
    "Changement numéro M. Dupont +33663462732 -> +33664472833\n",
    "Appel à maintenance : confirmer bascule en cours\n",
    "Changement de poste\n",
    "Faire : Bascule sur le réseau RN19\n",
    "Appeler la maintenance sur place\n",
    "Prévoir une solution à l'augmentation du flux\"\"\"\n",
    "text2 = \"\"\"Incident ligne 8\n",
    "Appeler Martin au 0768338833\n",
    "Transformateur à fouet\n",
    "Confirmation\"\"\"\n",
    "text3 =\"\"\"Prévoir retrait de la liaison Caen-Cherbourg\n",
    "Surveiller conso entre Nantes et Vannes\n",
    "Valeurs alarmantes de 500 MV\n",
    "-> MNV à réaliser SUAV\n",
    "RACR Deauville 63kV\"\"\"\n",
    "text1bis = \"\"\"Incident: coupure focale 140kV (poste PN-2)\n",
    "MV à faire toutes les 15 min\n",
    "Changement numéro M. Dupont +33663462732\n",
    "G +33664472833\n",
    "Appel à maintenance : confirmer bascule en cours\n",
    "Changement de poste\n",
    "Faire : Bascule sur le réseau RN19\n",
    "Appeler la maintenance sur place\n",
    "Préparer une solution à l'augmentation du flux\"\"\"\n",
    "text2bis = \"\"\"Incident S5 8\n",
    "Appeler Martin au 0768938899\n",
    "Transformateur ajouté\n",
    "Confirmation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "689b59ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reflow applied to: reflow_text1, reflow_text2, reflow_text3, reflow_text1bis, reflow_text2bis\n"
     ]
    }
   ],
   "source": [
    "# Applique `reflow_sentences` uniquement aux variables listées\n",
    "_reflow_target_names = ['text1', 'text2', 'text3', 'text1bis', 'text2bis']\n",
    "_reflow_vars = []\n",
    "_missing = []\n",
    "\n",
    "for _name in _reflow_target_names:\n",
    "    if _name in globals() and isinstance(globals()[_name], str):\n",
    "        tgt = f\"reflow_{_name}\"\n",
    "        globals()[tgt] = reflow_sentences(globals()[_name], width=80)\n",
    "        _reflow_vars.append(tgt)\n",
    "    else:\n",
    "        _missing.append(_name)\n",
    "\n",
    "# dictionnaire récapitulatif accessible dans le notebook\n",
    "reflowed_texts = {name: globals()[name] for name in _reflow_vars}\n",
    "\n",
    "# affiche un résumé bref\n",
    "print(\"Reflow applied to:\", \", \".join(_reflow_vars))\n",
    "if _missing:\n",
    "    print(\"Missing or non-str vars skipped:\", \", \".join(_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d2a2cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "text1 (original):\n",
      "Incident: coupure focale 140kV (poste PN-2)\n",
      "MV à faire toutes les 15 min\n",
      "Changement numéro M. Dupont +33663462732 -> +33664472833\n",
      "Appel à maintenance : confirmer bascule en cours\n",
      "Changement de poste\n",
      "Faire : Bascule sur le réseau RN19\n",
      "Appeler la maintenance sur place\n",
      "Prévoir une solution à l'augmentation du flux\n",
      "--------------------------------------------------------------------------------\n",
      "reflow_text1 (reflow):\n",
      "Incident: coupure focale 140kV (poste PN-2). MV à faire toutes les 15 min.\n",
      "Changement numéro M. Dupont +33663462732 -> +33664472833. Appel à maintenance :\n",
      "confirmer bascule en cours. Changement de poste. Faire : Bascule sur le réseau\n",
      "RN19. Appeler la maintenance sur place. Prévoir une solution à l'augmentation du\n",
      "flux.\n",
      "================================================================================\n",
      "text2 (original):\n",
      "Incident ligne 8\n",
      "Appeler Martin au 0768338833\n",
      "Transformateur à fouet\n",
      "Confirmation\n",
      "--------------------------------------------------------------------------------\n",
      "reflow_text2 (reflow):\n",
      "Incident ligne 8. Appeler Martin au 0768338833. Transformateur à fouet.\n",
      "Confirmation.\n",
      "================================================================================\n",
      "text3 (original):\n",
      "Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "Valeurs alarmantes de 500 MV\n",
      "-> MNV à réaliser SUAV\n",
      "RACR Deauville 63kV\n",
      "--------------------------------------------------------------------------------\n",
      "reflow_text3 (reflow):\n",
      "Prévoir retrait de la liaison Caen-Cherbourg. Surveiller conso entre Nantes et\n",
      "Vannes. Valeurs alarmantes de 500 MV. -> MNV à réaliser SUAV. RACR Deauville\n",
      "63kV.\n",
      "================================================================================\n",
      "text1bis (original):\n",
      "Incident: coupure focale 140kV (poste PN-2)\n",
      "MV à faire toutes les 15 min\n",
      "Changement numéro M. Dupont +33663462732\n",
      "G +33664472833\n",
      "Appel à maintenance : confirmer bascule en cours\n",
      "Changement de poste\n",
      "Faire : Bascule sur le réseau RN19\n",
      "Appeler la maintenance sur place\n",
      "Préparer une solution à l'augmentation du flux\n",
      "--------------------------------------------------------------------------------\n",
      "reflow_text1bis (reflow):\n",
      "Incident: coupure focale 140kV (poste PN-2). MV à faire toutes les 15 min.\n",
      "Changement numéro M. Dupont +33663462732. G +33664472833. Appel à maintenance :\n",
      "confirmer bascule en cours. Changement de poste. Faire : Bascule sur le réseau\n",
      "RN19. Appeler la maintenance sur place. Préparer une solution à l'augmentation\n",
      "du flux.\n",
      "================================================================================\n",
      "text2bis (original):\n",
      "Incident S5 8\n",
      "Appeler Martin au 0768938899\n",
      "Transformateur ajouté\n",
      "Confirmation\n",
      "--------------------------------------------------------------------------------\n",
      "reflow_text2bis (reflow):\n",
      "Incident S5 8. Appeler Martin au 0768938899. Transformateur ajouté.\n",
      "Confirmation.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "_pairs = [\n",
    "    ('text1', 'reflow_text1'),\n",
    "    ('text2', 'reflow_text2'),\n",
    "    ('text3', 'reflow_text3'),\n",
    "    ('text1bis', 'reflow_text1bis'),\n",
    "    ('text2bis', 'reflow_text2bis'),\n",
    "]\n",
    "\n",
    "for orig_name, reflow_name in _pairs:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{orig_name} (original):\")\n",
    "    orig = globals().get(orig_name)\n",
    "    if orig is None:\n",
    "        print(f\"<missing {orig_name}>\")\n",
    "    else:\n",
    "        print(orig)\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{reflow_name} (reflow):\")\n",
    "    # prefer direct var, fallback to reflowed_texts dict if present\n",
    "    reflow = globals().get(reflow_name)\n",
    "    if reflow is None and 'reflowed_texts' in globals():\n",
    "        reflow = globals()['reflowed_texts'].get(reflow_name)\n",
    "    if reflow is None:\n",
    "        print(f\"<missing {reflow_name}>\")\n",
    "    else:\n",
    "        print(reflow)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e49d3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise similarity (Jaccard, F1) + score/category:\n",
      "reflow_text1  <>  reflow_text2 -> jaccard=0.061, f1=0.100, score=0.081, category=different, phone_ok=False\n",
      "reflow_text1  <>  reflow_text3 -> jaccard=0.083, f1=0.135, score=0.109, category=different, phone_ok=False\n",
      "reflow_text1  <>  reflow_text1bis -> jaccard=0.930, f1=0.970, score=0.95, category=identical, phone_ok=True\n",
      "reflow_text1  <>  reflow_text2bis -> jaccard=0.041, f1=0.068, score=0.054, category=different, phone_ok=False\n",
      "reflow_text2  <>  reflow_text3 -> jaccard=0.029, f1=0.056, score=0.042, category=different, phone_ok=False\n",
      "reflow_text2  <>  reflow_text1bis -> jaccard=0.060, f1=0.098, score=0.079, category=different, phone_ok=False\n",
      "reflow_text2  <>  reflow_text2bis -> jaccard=0.500, f1=0.667, score=0.583, category=related, phone_ok=False\n",
      "reflow_text3  <>  reflow_text1bis -> jaccard=0.065, f1=0.107, score=0.086, category=different, phone_ok=False\n",
      "reflow_text3  <>  reflow_text2bis -> jaccard=0.000, f1=0.000, score=0.0, category=different, phone_ok=False\n",
      "reflow_text1bis  <>  reflow_text2bis -> jaccard=0.040, f1=0.067, score=0.053, category=different, phone_ok=False\n",
      "\n",
      "Résumé stocké dans la variable `reflow_pairwise_similarity`.\n"
     ]
    }
   ],
   "source": [
    "# Applique les métriques (Jaccard, token-F1) et la fonction score_and_categorize_texts\n",
    "# aux 5 textes reflow (pas les originaux) et affiche un résumé pairwise.\n",
    "\n",
    "reflow_names = ['reflow_text1', 'reflow_text2', 'reflow_text3', 'reflow_text1bis', 'reflow_text2bis']\n",
    "\n",
    "# Récupère les textes disponibles\n",
    "reflow_texts = {}\n",
    "missing = []\n",
    "for n in reflow_names:\n",
    "    v = globals().get(n)\n",
    "    if isinstance(v, str):\n",
    "        reflow_texts[n] = v\n",
    "    else:\n",
    "        missing.append(n)\n",
    "\n",
    "if missing:\n",
    "    print(\"Warning - variables manquantes ou non-str ignorées:\", \", \".join(missing))\n",
    "\n",
    "names = list(reflow_texts.keys())\n",
    "texts = [reflow_texts[n] for n in names]\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Pairwise similarity (Jaccard, F1) + score/category:\")\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        ni, nj = names[i], names[j]\n",
    "        ti, tj = texts[i], texts[j]\n",
    "\n",
    "        # tokens via helper déjà défini dans le notebook\n",
    "        tai = canonicalize_for_compare(ti)\n",
    "        tbj = canonicalize_for_compare(tj)\n",
    "\n",
    "        ja = token_jaccard(tai, tbj)\n",
    "        fa = token_f1(tai, tbj)\n",
    "        sc = score_and_categorize_texts(ti, tj)\n",
    "\n",
    "        results[(ni, nj)] = {\n",
    "            \"jaccard\": round(ja, 3),\n",
    "            \"f1\": round(fa, 3),\n",
    "            \"score\": sc[\"score\"],\n",
    "            \"category\": sc[\"category\"],\n",
    "            \"phone_ok\": sc.get(\"phone_ok\", False)\n",
    "        }\n",
    "\n",
    "        print(f\"{ni}  <>  {nj} -> jaccard={ja:.3f}, f1={fa:.3f}, score={sc['score']}, category={sc['category']}, phone_ok={sc.get('phone_ok', False)}\")\n",
    "\n",
    "# Rendre le dictionnaire accessible dans le notebook\n",
    "reflow_pairwise_similarity = results\n",
    "print(\"\\nRésumé stocké dans la variable `reflow_pairwise_similarity`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ff7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"Prévair retrait de la liaison Caen-Cherbourg\n",
    "Surveiller conso entre Nantes et Vannes\n",
    "Valeurs alarmantes de 500 MV\n",
    "-> MNV à réaliser SUAV\n",
    "RACR Deauville 63kV\n",
    "Bascule sur Caen\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.92\n",
    "text1bis =\"\"\" Prévoir retrait de la liaison Caen-Cherbourg\n",
    "Surveiller conso entre Nantes et Vannes\n",
    "Valeurs alarmantes de 500 MV\n",
    "-> MNV à réaliser 5 SUAV\n",
    "RACR Deauville 63kV\n",
    "Bascule sur Caen\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.945\n",
    "text1ter = \"\"\" Prévoir retrait de la liaison Caen - Cherbourg\n",
    "Surveiller conso entre Nantes et Vannes\n",
    "Valeurs alarmantes de 500 MV\n",
    "-> MNV à réaliser SUAV\n",
    "RACR Deauville 63kV\n",
    "Bascule sur Caen\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c78129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.788\n",
    "text1nv = \"\"\" Prévoir retrait de la liaison Caen-Cherbourg\n",
    "Surveiller conso entre Nantes et Vannes\n",
    "Valeurs alarmantes de 500 MV\n",
    "-> MNV à réaliser SUAV\n",
    "RACR Deauville 63kV\n",
    "Bascule sur Caen\n",
    "Retour à la normale\n",
    "Blandissement de la tension\n",
    "Chargement ensuite\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\" Prévoir retrait de la liaison Caen-Cherbourg appel COSE-P à prévoir\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2de3843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reflow text1 ===\n",
      "Incident: coupure focale 140kV (poste PN-2) MV à faire toutes les 15 min.\n",
      "Changement numéro M. Dupont. +33663462732. G +33664472833. Appel à maintenance:\n",
      "confirmer bascule en œuvre. Changement de poste. Faire: Bascule sur le réseau\n",
      "RN19. Appeler la maintenance sur place. Prévoir une solution à l'augmentation du\n",
      "flux.\n",
      "\n",
      "=== Reflow text2 ===\n",
      "Confirmer la MNV SNCF Nantes-Vannes. Étude RACR Caen Cherbourg 225kV. Analyse\n",
      "des audios Cherbourg. Changement de numéro +33663962732. Penser à basculer sur\n",
      "le N-1 de Strasbourg-Colmar à 14h31.\n",
      "\n",
      "=== Score & catégorisation ===\n",
      "{'score': 0.7, 'category': 'related', 'jaccard': 0.129, 'f1': 0.222, 'phone_ok': True}\n"
     ]
    }
   ],
   "source": [
    "# Reflow et score entre text1 et text2 (nouvelle cellule)\n",
    "reflow_cell25_text1 = reflow_sentences(text1, width=80)\n",
    "reflow_cell25_text2 = reflow_sentences(text2, width=80)\n",
    "\n",
    "print(\"=== Reflow text1 ===\")\n",
    "print(reflow_cell25_text1)\n",
    "print(\"\\n=== Reflow text2 ===\")\n",
    "print(reflow_cell25_text2)\n",
    "\n",
    "# calcul du score / catégorisation entre les deux reflows\n",
    "score_cell25 = score_and_categorize_texts(reflow_cell25_text1, reflow_cell25_text2)\n",
    "print(\"\\n=== Score & catégorisation ===\")\n",
    "print(score_cell25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
