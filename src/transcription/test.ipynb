{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "from pathlib import Path\n",
    "import re\n",
    "# Remplace \"TON_TOKEN\" par ton token Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "\n",
    "def VAD(audio_path, min_duration_on=2, min_duration_of=2):\n",
    "    \n",
    "    model = Model.from_pretrained(\n",
    "    \"pyannote/segmentation-3.0\")\n",
    "\n",
    "    pipeline = VoiceActivityDetection(segmentation=model)\n",
    "    HYPER_PARAMETERS = {\n",
    "    # Si un segment de parole détecté dure moins de 3 secondes, il sera ignoré.\n",
    "    \"min_duration_on\": min_duration_on,\n",
    "    # Si une pause est plus courte que 10 secondes, elle peut être remplie ou fusionnée avec les segments voisins.\n",
    "    \"min_duration_off\": min_duration_of\n",
    "    }\n",
    "    pipeline.instantiate(HYPER_PARAMETERS)\n",
    "    vad = pipeline(audio_path)\n",
    "    segment_start = []\n",
    "    segment_end = []\n",
    "    for segment, _, _ in vad.itertracks(yield_label=True):\n",
    "        #print(segment)\n",
    "        segment_start.append(segment.start)\n",
    "        segment_end.append(segment.end)\n",
    "    return segment_start,segment_end\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e98c241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 00:00:00.030 -->  00:00:35.603]\n",
      "[ 00:00:35.822 -->  00:00:42.640]\n",
      "([0.03096875, 35.822843750000004], [35.603468750000005, 42.64034375])\n"
     ]
    }
   ],
   "source": [
    "folder = Path(\"tmp\")\n",
    "\n",
    "for audio_path in folder.glob(\"*.wav\"): \n",
    "    print(VAD(audio_path, min_duration_on=0, min_duration_of=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c2c83f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00:00:00.030', '00:00:35.603']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ligne = \"[ 00:00:00.030 -->  00:00:35.603]\"\n",
    "segments = re.findall(r\"\\d{2}:\\d{2}:\\d{2}\\.\\d{3}\", ligne)\n",
    "print(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35c16b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedlbakali/Desktop/DTY/detection-notes/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/mohammedlbakali/Desktop/DTY/detection-notes/.venv/lib/python3.12/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n"
     ]
    }
   ],
   "source": [
    "folder = Path(\"tests\")\n",
    "\n",
    "for audio_path in folder.glob(\"*.wav\"): \n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "start = int(30 * sr)\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20124317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedlbakali/Desktop/DTY/detection-notes/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "/Users/mohammedlbakali/Desktop/DTY/detection-notes/.venv/lib/python3.12/site-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
      "/Users/mohammedlbakali/Desktop/DTY/detection-notes/.venv/lib/python3.12/site-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n",
      "/Users/mohammedlbakali/Desktop/DTY/detection-notes/.venv/lib/python3.12/site-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "start = int(0 * sr)\n",
    "end = int(30*4 * sr)\n",
    "\n",
    "segment = waveform[:, start:end]\n",
    "\n",
    "segment_filename = f\"{os.path.splitext(os.path.basename(audio_path))[0]}_segment_{0}.wav\"\n",
    "segment_path = os.path.join(\"tests\", segment_filename)\n",
    "\n",
    "torchaudio.save(segment_path, segment, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "import librosa\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "def diarization(audio_path, num_speakers=None, min_speakers=None, max_speakers=None):\n",
    "    \n",
    "    if num_speakers is not None and (min_speakers is not None or max_speakers is not None):\n",
    "        raise ValueError(\"Tu ne peux pas utiliser à la fois 'num_speakers' et ('min_speakers' ou 'max_speakers').\")\n",
    "\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-community-1\", \n",
    "        token=\"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\")\n",
    "\n",
    "    if num_speakers is not None:\n",
    "        output = pipeline(audio_path, num_speakers=num_speakers)\n",
    "        \n",
    "    elif min_speakers is not None or max_speakers is not None:\n",
    "        output = pipeline(audio_path, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "    else:\n",
    "        output = pipeline(audio_path)\n",
    "        \n",
    "    diarization = output.speaker_diarization\n",
    "\n",
    "    list_start = []\n",
    "    list_end = []\n",
    "    list_speaker = []\n",
    "    \n",
    "    segment_start = []\n",
    "    segment_end = []\n",
    "    segment_speaker = []\n",
    "    \n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "\n",
    "        list_start.append(turn.start)\n",
    "        list_end.append(turn.end)\n",
    "        list_speaker.append(speaker)\n",
    "        \n",
    "        if len(list_speaker) == 1:\n",
    "            list_speaker.append(speaker) \n",
    "\n",
    "        if list_speaker[-2] != speaker and list_end[-2]-list_start[0] > 1:\n",
    "            \n",
    "            segment_start.append(list_start[0])\n",
    "            segment_end.append(list_end[-2])\n",
    "            segment_speaker.append(list_speaker[-2])\n",
    "            \n",
    "            #print(f\"{list_start[0]:.1f}s - {list_end[-2]:.1f}s : {list_speaker[-2]}\")\n",
    "            \n",
    "            list_start = [list_start[-1]]\n",
    "            list_end = [list_end[-1]]\n",
    "            list_speaker = [list_speaker[-1]]\n",
    "            \n",
    "    segment_start.append(list_start[0])\n",
    "    segment_end.append(list_end[-1])\n",
    "    segment_speaker.append(list_speaker[-1])\n",
    "    \n",
    "    #print(f\"{list_start[0]:.1f}s - {list_end[-1]:.1f}s : {list_speaker[-1]}\")\n",
    "    \n",
    "    return segment_start, segment_end, segment_speaker\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "300073ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests/ia_podcast_segment_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedlbakali/Desktop/DTY/detection-notes/.venv/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:103: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4s - 5.2s : SPEAKER_00\n",
      "5.9s - 14.8s : SPEAKER_03\n",
      "14.8s - 52.2s : SPEAKER_03\n",
      "52.2s - 76.1s : SPEAKER_02\n",
      "77.4s - 113.0s : SPEAKER_01\n",
      "113.2s - 120.0s : SPEAKER_02\n",
      "([2.3597187500000003, 5.85284375, 14.83034375, 52.22534375, 77.35221875, 113.19471875], [5.19471875, 14.83034375, 52.22534375, 76.06971875, 112.97534375000001, 119.97846875], ['SPEAKER_00', 'SPEAKER_03', 'SPEAKER_03', 'SPEAKER_02', 'SPEAKER_01', 'SPEAKER_02'])\n"
     ]
    }
   ],
   "source": [
    "folder = Path(\"tests\")\n",
    "\n",
    "for audio_path in folder.glob(\"*.wav\"): \n",
    "    print(audio_path)\n",
    "    print(diarization(audio_path))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470d537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
